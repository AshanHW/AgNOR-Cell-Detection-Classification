{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Pipeline\n",
    "\n",
    "### Detection + Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the previously trained detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = retinanet_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # background=0 , cell=1\n",
    "in_features = detection_model.backbone.out_channels\n",
    "\n",
    "# Anchor boxes\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5,\n",
    ")\n",
    "\n",
    "num_anchors = anchor_generator.num_anchors_per_location()[0]\n",
    "detection_model.head = torchvision.models.detection.retinanet.RetinaNetHead(\n",
    "    in_channels=in_features, num_classes=num_classes, num_anchors=num_anchors\n",
    ")\n",
    "\n",
    "detection_model.anchor_generator = anchor_generator\n",
    "detection_model.to(device)\n",
    "detection_model.load_save_dict(torch.load(\"replace w/ path to weights\"))\n",
    "detection_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the previously trained classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_model = efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "\n",
    "\n",
    "# Freezing layers\n",
    "for param in classify_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classes = 12\n",
    "classify_model.classifier[-1] = torch.nn.Linear(in_features=1280, out_features=classes)\n",
    "\n",
    "classify_model.to(device)\n",
    "classify_model.load_state_dict(torch.load('replace w/ path to weights', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs images through the detection network and get the detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crops(image, crop_size, overlap):\n",
    "    img_w, img_h = image.size\n",
    "\n",
    "    step_size = int(crop_size - crop_size * overlap / 100)\n",
    "\n",
    "    crops = []\n",
    "\n",
    "    tr = transforms.ToTensor()\n",
    "\n",
    "    for y in range(0,img_h - crop_size + 1, step_size):\n",
    "        for x in range(0, img_w - crop_size +1, step_size):\n",
    "\n",
    "            if (x + crop_size <= img_w) and (y + crop_size <= img_h):\n",
    "                cropped = image.crop(((x, y,x+crop_size, y+crop_size)))\n",
    "\n",
    "                crops.append(tr(cropped))\n",
    "\n",
    "\n",
    "    return crops\n",
    "\n",
    "\n",
    "def process_detections(detections, detection_threshold):\n",
    "   results = []\n",
    "\n",
    "   for detection in detections:\n",
    "      boxes = detection['boxes']\n",
    "      scores = detection['scores']\n",
    "      labels = detection['labels']\n",
    "\n",
    "      boxes = boxes.cpu()\n",
    "      scores = scores.cpu()\n",
    "      labels = labels.cpu()\n",
    "\n",
    "      keep = nms(boxes, scores, detection_threshold)\n",
    "\n",
    "      results.append({\n",
    "         'boxes': boxes[keep],\n",
    "         'scores' : scores[keep],\n",
    "         'labels' : labels[keep]\n",
    "      })\n",
    "\n",
    "      return results\n",
    "\n",
    "\n",
    "def process_image(image, crop_size, overlap, model, detection_threshold):\n",
    "    crops = get_crops(image, crop_size, overlap)\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for crop in crops:\n",
    "      crop_tensor = [crop.float().to(device)]\n",
    "      with torch.no_grad():\n",
    "        out = model(crop_tensor)\n",
    "        outputs.extend(out)\n",
    "\n",
    "    final_output = process_detections(outputs, detection_threshold)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs detections through the classification network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cells(image, cords, clas_model, crop_size=50):\n",
    "    clas_model.eval()\n",
    "    outputs = []\n",
    "    tr = transforms.Compose([\n",
    "        transforms.Resize((crop_size, crop_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    for cord in cords:\n",
    "        cord = tuple(cord)\n",
    "        cropped = image.crop((cord))\n",
    "        \n",
    "        cropped = tr(cropped).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = clas_model(cropped)\n",
    "            probability = F.softmax(out, dim=1)\n",
    "            outputs.append(probability)\n",
    "\n",
    "    results = {} # {label:[cells]}\n",
    "    agNOR = 0\n",
    "    predicted_classes = [torch.argmax(pred).item() for pred in outputs]\n",
    "\n",
    "    for i in range(len(predicted_classes)):\n",
    "        if predicted_classes[i] in results:\n",
    "            results[predicted_classes[i]].append(cords[i])\n",
    "        else:\n",
    "            results[predicted_classes[i]] = [cords[i]]\n",
    "        agNOR += predicted_classes[i]\n",
    "\n",
    "    agNOR /= len(predicted_classes)\n",
    "    return results, agNOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AgNOR_score(image, detect_model, clas_model, crop_size, overlap, detection_threshold):\n",
    "    cell_detections = process_image(image, crop_size, overlap, detect_model, detection_threshold)\n",
    "    cell_cordinates = cell_detections[0]['boxes'].tolist()\n",
    "\n",
    "    cell_label_dict, agnor_score = process_cells(image, cell_cordinates, clas_model)\n",
    "\n",
    "    return cell_label_dict, agnor_score"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
