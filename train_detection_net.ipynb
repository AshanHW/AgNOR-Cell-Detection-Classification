{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetinaNet_ResNet50_fpn Network for Cell detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset class\n",
    "\n",
    "In the classification network, we returned a cell at time with get_item method. In the detection network, we will return a larger crop of an image (300 x 300). Also, we have to include the bounding boxes and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, transform, num_samples=10, crop_width = 300, crop_height = 300):\n",
    "        self.df = df\n",
    "        self.num_samples = num_samples # number of crops\n",
    "        self.crop_width = crop_width\n",
    "        self.crop_height = crop_height\n",
    "        self.transform = transform\n",
    "        self.image_names = df['filename'].unique()\n",
    "\n",
    "\n",
    "    def is_within_crop(self, x, y, crop_x, crop_y, crop_w, crop_h):\n",
    "      return crop_x <= x < crop_x + crop_w and crop_y <= y < crop_y + crop_h\n",
    "\n",
    "    def adjust_coordinates(self, x, y, crop_x, crop_y):\n",
    "      return x - crop_x, y - crop_y\n",
    "\n",
    "    def clamp_bbox(self, bbox, width, height):\n",
    "        x_min, y_min, x_max, y_max, label = bbox\n",
    "        x_min = max(0, min(x_min, width - 1))\n",
    "        y_min = max(0, min(y_min, height - 1))\n",
    "        x_max = max(0, min(x_max, width - 1))\n",
    "        y_max = max(0, min(y_max, height - 1))\n",
    "        return [x_min, y_min, x_max, y_max, label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names) * self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "      # selecting an image and a crop indexes\n",
    "      img_index = index // self.num_samples\n",
    "      crop_index = index % self.num_samples\n",
    "\n",
    "      img_name = self.image_names[img_index]\n",
    "      img_path = os.path.join('replace w/ path', img_name)\n",
    "      image = cv2.imread(img_path)\n",
    "\n",
    "      height, width, _ = image.shape\n",
    "\n",
    "      # cropping randomly\n",
    "      crop_x = np.random.randint(0, width - self.crop_width + 1)\n",
    "      crop_y = np.random.randint(0, height - self.crop_height + 1)\n",
    "      crop_image = image[crop_y:crop_y + self.crop_height, crop_x:crop_x + self.crop_width]\n",
    "      crop_image = cv2.cvtColor(crop_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "      # cells for the current image\n",
    "      img_df = self.df[self.df['filename'] == img_name]\n",
    "      crop_annotations = []\n",
    "\n",
    "      for _, row in img_df.iterrows():\n",
    "        max_x, max_y = row['max_x'], row['max_y']\n",
    "        min_x, min_y = row['min_x'], row['min_y']\n",
    "        label = row['label']\n",
    "\n",
    "        if (self.is_within_crop(max_x, max_y, crop_x, crop_y, self.crop_width, self.crop_height) or\n",
    "            self.is_within_crop(min_x, min_y, crop_x, crop_y, self.crop_width, self.crop_height) or\n",
    "            self.is_within_crop(min_x, max_y, crop_x, crop_y, self.crop_width, self.crop_height) or\n",
    "            self.is_within_crop(max_x, min_y, crop_x, crop_y, self.crop_width, self.crop_height)):\n",
    "\n",
    "          adj_max_x, adj_max_y = self.adjust_coordinates(max_x, max_y, crop_x, crop_y)\n",
    "          adj_min_x, adj_min_y = self.adjust_coordinates(min_x, min_y, crop_x, crop_y)\n",
    "\n",
    "          clamped_bbox = self.clamp_bbox([adj_min_x, adj_min_y, adj_max_x, adj_max_y, label], self.crop_width, self.crop_height)\n",
    "          #crop_annotations.append(clamped_bbox)\n",
    "\n",
    "          if clamped_bbox[0] < clamped_bbox[2] and clamped_bbox[1] < clamped_bbox[3]:\n",
    "                    crop_annotations.append(clamped_bbox)\n",
    "\n",
    "      # normalising bboxes[0,1] to be compatible with albumentaions\n",
    "      bboxes = []\n",
    "      labels = []\n",
    "      for an in crop_annotations:\n",
    "        x_min, y_min, x_max, y_max, label = an\n",
    "\n",
    "        if x_min < x_max and y_min < y_max:\n",
    "          if label != 0:\n",
    "            bboxes.append([\n",
    "                min(x_min / self.crop_width, x_max / self.crop_width),\n",
    "                min(y_min / self.crop_height, y_max / self.crop_height),\n",
    "                max(x_min / self.crop_width, x_max / self.crop_width),\n",
    "                max(y_min / self.crop_height, y_max / self.crop_height),\n",
    "            ])\n",
    "            labels.append(1)\n",
    "          else:\n",
    "            bboxes.append([\n",
    "                min(x_min / self.crop_width, x_max / self.crop_width),\n",
    "                min(y_min / self.crop_height, y_max / self.crop_height),\n",
    "                max(x_min / self.crop_width, x_max / self.crop_width),\n",
    "                max(y_min / self.crop_height, y_max / self.crop_height),\n",
    "            ])\n",
    "            labels.append(0)\n",
    "      # Ensure at least one bounding box is present\n",
    "      if len(bboxes) == 0:\n",
    "        bboxes.append([0, 0, 1, 1])  # Default bounding box\n",
    "        labels.append(0)\n",
    "\n",
    "      # Applying transormations\n",
    "      transformed = self.transform(image= crop_image,\n",
    "                                   bboxes= bboxes,\n",
    "                                   labels= labels)\n",
    "      crop_image = transformed['image']\n",
    "      transformed_bboxes = transformed['bboxes']\n",
    "      labels = transformed['labels']\n",
    "\n",
    "      target = {\n",
    "          'boxes': torch.tensor(bboxes, dtype=torch.float32),\n",
    "          'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "      }\n",
    "\n",
    "      return torch.tensor(crop_image, dtype=torch.float32).permute(2,0,1), target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Methods\n",
    "\n",
    "Geometric augmentations are complicated for detection tasks, as bboxes always have to be transformed too. Therefore, we use albumentations library to solve this issue.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=90, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the model\n",
    "\n",
    "For this task, we utilise retinanet_resnet50_fpn model, which is a retinanet with resnet50_fpn backbone. Retinanet uses anchor boxes to predict the bounding box. The model also use focal loss function, hence we don't need to define a custom function.\n",
    "\n",
    "More information about the model can be found on torchvision documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = retinanet_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # background=0 , cell=1\n",
    "in_features = model.backbone.out_channels\n",
    "\n",
    "# Anchor boxes\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5,\n",
    ")\n",
    "\n",
    "num_anchors = anchor_generator.num_anchors_per_location()[0]\n",
    "model.head = torchvision.models.detection.retinanet.RetinaNetHead(\n",
    "    in_channels=in_features, num_classes=num_classes, num_anchors=num_anchors\n",
    ")\n",
    "\n",
    "\n",
    "model.anchor_generator = anchor_generator\n",
    "\n",
    "# Freezing the backbone\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer & Splitting the dataset\n",
    "\n",
    "Adam optimizer\n",
    "\n",
    "Train 70 % , Validation 15 %, Test 15 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = pd.read_csv('annotation_frame.csv')\n",
    "\n",
    "traindata, testdata = train_test_split(my_dataset, test_size=0.3, random_state=56)\n",
    "validata, testdata = train_test_split(testdata, test_size=0.5, random_state=56)\n",
    "\n",
    "train_dataset = MyDataset(traindata, transform=transform)\n",
    "val_dataset = MyDataset(validata, transform=transform)\n",
    "test_dataset = MyDataset(testdata, transform=transform)\n",
    "\n",
    "def custom_length(batch):\n",
    "  images, targets = zip(*batch)\n",
    "  return images, targets\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=4, collate_fn=custom_length, shuffle=True)\n",
    "valiloader = DataLoader(val_dataset, batch_size=4, collate_fn=custom_length, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=4, collate_fn=custom_length, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validating Loops\n",
    "\n",
    "We'll use inbuilt focal loss as the loss function. But we need a detection metric to evaluate the accuracy of the bounding boxes. Therefore, we will use mean average precision (mAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc='Training'):\n",
    "      images = torch.stack(inputs)\n",
    "      targets = [{'boxes': t['boxes'], 'labels': t['labels']} for t in targets]\n",
    "\n",
    "      # making gradients zero\n",
    "      optimizer.zero_grad()\n",
    "      # forward pass\n",
    "      loss_dict = model(images, targets)\n",
    "      losses = sum(loss for loss in loss_dict.values())\n",
    "      #back propagation\n",
    "      losses.backward()\n",
    "      # updating parameters\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += losses.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "def valid_one_epoch(model, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    mAP = MeanAveragePrecision(iou_type='bbox')\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc='Validation'):\n",
    "          images = torch.stack(inputs)\n",
    "          targets = [{'boxes': t['boxes'], 'labels': t['labels']} for t in targets]\n",
    "\n",
    "          # Calculate loss\n",
    "          loss_dict = model(images, targets)\n",
    "          losses = sum(loss for loss in loss_dict.values())\n",
    "          val_loss += losses.item()\n",
    "          # Collect predictions and targets for mAP calculation\n",
    "          outputs = model(images)\n",
    "          for output, target in zip(outputs, targets):\n",
    "                all_predictions.append({k: v for k, v in output.items()})\n",
    "                all_targets.append({k: v for k, v in target.items()})\n",
    "\n",
    "    # Calculate mAP\n",
    "    mAP_val = mAP(all_predictions, all_targets)\n",
    "\n",
    "    epoch_loss = val_loss / len(dataloader)\n",
    "\n",
    "    return epoch_loss, mAP_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_epoch(model, train_loader, valid_loader, optimizer, n_epochs=5):\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  valid_mAPs = []\n",
    "  best_mAP = -1\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}')\n",
    "\n",
    "    # Training\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    # Validation\n",
    "    valid_loss, mAP_val = valid_one_epoch(model, valid_loader)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_mAPs.append(mAP_val)\n",
    "    print(f\"Validation Loss: {valid_loss}\")\n",
    "    print(f\"Mean Average Precision: {mAP_val}\")\n",
    "\n",
    "    if mAP_val > best_mAP:\n",
    "            best_mAP = mAP_val\n",
    "            torch.save(model.state_dict(), 'best_model_detection.pth')\n",
    "\n",
    "\n",
    "  return train_losses, valid_losses, valid_mAPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses, valid_mAPs = n_epoch(model, trainloader, valiloader, optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
